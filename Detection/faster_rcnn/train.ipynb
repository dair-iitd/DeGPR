{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic python and ML Libraries\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# for ignoring warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# We will be reading images using OpenCV\n",
    "import cv2\n",
    "\n",
    "# xml library for parsing xml files\n",
    "from xml.etree import ElementTree as et\n",
    "\n",
    "# matplotlib for visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "# torchvision libraries\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision import transforms as torchtrans  \n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "\n",
    "# these are the helper libraries imported.\n",
    "from engine import train_one_epoch, evaluate\n",
    "import utils\n",
    "import transforms as T\n",
    "\n",
    "# for image augmentations\n",
    "import albumentations as A\n",
    "from albumentations.pytorch.transforms import ToTensorV2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IELDataset(torch.utils.data.Dataset):\n",
    "\n",
    "    def __init__(self,images_dir, width, height, transforms=None):\n",
    "        self.transforms = transforms\n",
    "        self.images_dir = images_dir\n",
    "        self.label_dir = images_dir.replace(\"images\",\"labels\")\n",
    "        self.height = height\n",
    "        self.width = width\n",
    "        \n",
    "        # sorting the images for consistency\n",
    "        # To get images, the extension of the filename is checked to be jpg\n",
    "        self.imgs = [image for image in sorted(os.listdir(images_dir))\n",
    "                        if image[-4:]=='.jpg']\n",
    "        \n",
    "        for text_file in sorted(os.listdir(self.label_dir)):\n",
    "            l = 0\n",
    "            with open(os.path.join(self.label_dir,text_file),'r') as f:\n",
    "                for x in f:\n",
    "                    l += 1\n",
    "                    \n",
    "            if l == 0:\n",
    "                self.imgs.remove(text_file.replace('.txt','.jpg'))\n",
    "        \n",
    "        # classes: 0 index is reserved for background\n",
    "        self.classes = [_, 'Epithelial Nuclei','IEL']\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        img_name = self.imgs[idx]\n",
    "        image_path = os.path.join(self.images_dir, img_name)\n",
    "\n",
    "        # reading the images and converting them to correct size and color    \n",
    "        img = cv2.imread(image_path)\n",
    "        img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB).astype(np.float32)\n",
    "        img_res = cv2.resize(img_rgb, (self.width, self.height), cv2.INTER_CUBIC)\n",
    "        # diving by 255\n",
    "        img_res /= 255.0\n",
    "        \n",
    "        boxes = []\n",
    "        labels = []\n",
    "        \n",
    "        wt = img.shape[1]\n",
    "        ht = img.shape[0]\n",
    "        \n",
    "        label_path = os.path.join(self.label_dir, img_name.replace('.jpg','.txt'))\n",
    "        \n",
    "        with open(label_path,'r') as f:\n",
    "            for line in f:\n",
    "                splits = line.split(' ')\n",
    "                w = float(splits[3]) * wt\n",
    "                h = float(splits[4]) * ht\n",
    "                x1 = ((2 * float(splits[1]) * wt) - w)/2\n",
    "                y1 = ((2 * float(splits[2]) * ht) - h)/2\n",
    "                x2 = x1 + w\n",
    "                y2 = y1 + h\n",
    "                \n",
    "                x1 = max(0,(x1/wt)*self.width)\n",
    "                x2 = min(self.width-1,(x2/wt)*self.width)\n",
    "                y1 = max(0,(y1/ht)*self.height)\n",
    "                y2 = min(self.height-1,(y2/ht)*self.height)\n",
    "                \n",
    "                if x1 >= x2 or y1 >= y2:\n",
    "                    continue\n",
    "                \n",
    "                boxes.append([x1,y1,x2,y2])\n",
    "                labels.append(int(splits[0]) + 1)\n",
    "                \n",
    "        boxes = [box for box in boxes if len(box) == 4]\n",
    "                \n",
    "        # convert boxes into a torch.Tensor\n",
    "        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
    "        \n",
    "        # getting the areas of the boxes\n",
    "        if boxes.shape[0] == 0:\n",
    "            area = boxes\n",
    "        else:\n",
    "            area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n",
    "\n",
    "        # suppose all instances are not crowd\n",
    "        iscrowd = torch.zeros((boxes.shape[0],), dtype=torch.int64)\n",
    "        \n",
    "        labels = torch.as_tensor(labels, dtype=torch.int64)\n",
    "        \n",
    "        target = {}\n",
    "        target[\"boxes\"] = boxes\n",
    "        target[\"labels\"] = labels\n",
    "        target[\"area\"] = area\n",
    "        target[\"iscrowd\"] = iscrowd\n",
    "        # image_id\n",
    "        image_id = torch.tensor([idx])\n",
    "        target[\"image_id\"] = image_id\n",
    "\n",
    "\n",
    "        if self.transforms:\n",
    "            \n",
    "            sample = self.transforms(image = img_res,\n",
    "                                     bboxes = target['boxes'],\n",
    "                                     labels = labels)\n",
    "            \n",
    "            img_res = sample['image']\n",
    "            target['boxes'] = torch.Tensor(sample['bboxes'])\n",
    "            \n",
    "        return img_res , target\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Send train=True fro training transforms and False for val/test transforms\n",
    "def get_transform(train):\n",
    "    \n",
    "    if train:\n",
    "        return A.Compose([\n",
    "                            A.HorizontalFlip(0.5),\n",
    "                     # ToTensorV2 converts image to pytorch tensor without div by 255\n",
    "                            ToTensorV2(p=1.0) \n",
    "                        ], bbox_params={'format': 'pascal_voc', 'label_fields': ['labels']})\n",
    "    else:\n",
    "        return A.Compose([\n",
    "                            ToTensorV2(p=1.0)\n",
    "                        ], bbox_params={'format': 'pascal_voc', 'label_fields': ['labels']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_object_detection_model(num_classes):\n",
    "\n",
    "    # load a model pre-trained pre-trained on COCO\n",
    "    model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "    \n",
    "    # get number of input features for the classifier\n",
    "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "    # replace the pre-trained head with a new one\n",
    "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes) \n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "fold_dir = \"/home/aayush/chirag/tensorflow/yolo_kfold/2/images/\"\n",
    "\n",
    "# use our dataset and defined transformations\n",
    "dataset_train = IELDataset(os.path.join(fold_dir,\"train\"), 640, 640, transforms= get_transform(train=True))\n",
    "dataset_val = IELDataset(os.path.join(fold_dir,\"val\"), 640, 640, transforms= get_transform(train=False))\n",
    "dataset_test = IELDataset(os.path.join(fold_dir,\"test\"), 640, 640, transforms= get_transform(train=False))\n",
    "\n",
    "# define training and validation data loaders\n",
    "data_loader_train = torch.utils.data.DataLoader(\n",
    "    dataset_train, batch_size=4, shuffle=True, num_workers=4,\n",
    "    collate_fn=utils.collate_fn)\n",
    "\n",
    "data_loader_val = torch.utils.data.DataLoader(\n",
    "    dataset_val, batch_size=4, shuffle=False, num_workers=4,\n",
    "    collate_fn=utils.collate_fn)\n",
    "\n",
    "data_loader_test = torch.utils.data.DataLoader(\n",
    "    dataset_test, batch_size=4, shuffle=False, num_workers=4,\n",
    "    collate_fn=utils.collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to train on gpu if selected.\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "num_classes = 3\n",
    "\n",
    "# get the model using our helper function\n",
    "model = get_object_detection_model(num_classes)\n",
    "\n",
    "# move model to the right device\n",
    "model.to(device)\n",
    "\n",
    "# construct an optimizer\n",
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer = torch.optim.SGD(params, lr=0.005,\n",
    "                            momentum=0.9, weight_decay=0.0005)\n",
    "\n",
    "# and a learning rate scheduler which decreases the learning rate by\n",
    "# 10x every 3 epochs\n",
    "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer,\n",
    "                                               step_size=3,\n",
    "                                               gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [0]  [ 0/49]  eta: 0:00:48  lr: 0.000109  loss: 5.8621 (5.8621)  loss_classifier: 0.8870 (0.8870)  loss_box_reg: 0.2426 (0.2426)  loss_objectness: 4.5299 (4.5299)  loss_rpn_box_reg: 0.2025 (0.2025)  time: 0.9963  data: 0.2757  max mem: 4622\n",
      "Epoch: [0]  [10/49]  eta: 0:00:28  lr: 0.001150  loss: 10.4576 (33.5209)  loss_classifier: 0.6585 (0.6988)  loss_box_reg: 0.6426 (0.5348)  loss_objectness: 0.4305 (1.7168)  loss_rpn_box_reg: 0.1331 (0.1489)  posterior_loss: 14.9156 (41.8298)  time: 0.7188  data: 0.0325  max mem: 6005\n",
      "Epoch: [0]  [20/49]  eta: 0:00:20  lr: 0.002190  loss: 21.0020 (7208657706.3437)  loss_classifier: 0.5561 (0.6209)  loss_box_reg: 0.7171 (0.6428)  loss_objectness: 0.2500 (0.9866)  loss_rpn_box_reg: 0.1248 (0.1396)  posterior_loss: 25.5840 (8410100601.0771)  time: 0.6892  data: 0.0088  max mem: 6017\n",
      "Epoch: [0]  [30/49]  eta: 0:00:13  lr: 0.003231  loss: 27.5865 (9498823425.0453)  loss_classifier: 0.5228 (0.5799)  loss_box_reg: 0.7548 (0.6742)  loss_objectness: 0.1247 (0.6984)  loss_rpn_box_reg: 0.1040 (0.1298)  posterior_loss: 31.4887 (11325520002.3666)  time: 0.6858  data: 0.0093  max mem: 6017\n",
      "Epoch: [0]  [40/49]  eta: 0:00:06  lr: 0.004272  loss: 4.6170 (7182037242.8955)  loss_classifier: 0.5020 (0.5605)  loss_box_reg: 0.7309 (0.6855)  loss_objectness: 0.0851 (0.5489)  loss_rpn_box_reg: 0.1034 (0.1255)  posterior_loss: 4.1332 (8179542245.2683)  time: 0.6879  data: 0.0097  max mem: 6018\n",
      "Epoch: [0]  [48/49]  eta: 0:00:00  lr: 0.005000  loss: 4.5135 (6009459735.2257)  loss_classifier: 0.4939 (0.5453)  loss_box_reg: 0.6953 (0.6778)  loss_objectness: 0.0829 (0.4728)  loss_rpn_box_reg: 0.1025 (0.1214)  posterior_loss: 3.2024 (6692352747.4247)  time: 0.6731  data: 0.0094  max mem: 6018\n",
      "Epoch: [0] Total time: 0:00:33 (0.6890 s / it)\n",
      "creating index...\n",
      "index created!\n",
      "Test:  [ 0/10]  eta: 0:00:05  model_time: 0.1968 (0.1968)  evaluator_time: 0.1164 (0.1164)  time: 0.5635  data: 0.2457  max mem: 6018\n",
      "Test:  [ 9/10]  eta: 0:00:00  model_time: 0.1933 (0.1804)  evaluator_time: 0.1200 (0.1190)  time: 0.3350  data: 0.0323  max mem: 6018\n",
      "Test: Total time: 0:00:03 (0.3413 s / it)\n",
      "Averaged stats: model_time: 0.1933 (0.1804)  evaluator_time: 0.1200 (0.1190)\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.03s).\n",
      "IoU metric: bbox\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.078\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.298\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.012\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.048\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.128\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.100\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.016\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.117\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.207\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.134\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.308\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.100\n"
     ]
    }
   ],
   "source": [
    "# training for 10 epochs\n",
    "num_epochs = 1\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # training for one epoch\n",
    "    train_one_epoch(model, optimizer, data_loader_train, device, epoch, print_freq=10,postreg=True)\n",
    "    # update the learning rate\n",
    "    lr_scheduler.step()\n",
    "    # evaluate on the test dataset\n",
    "    evaluate(model, data_loader_val, device=device)\n",
    "\n",
    "#torch.save(model, './weights/faster-rcnn-iel.pt')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "fce2ee5fb604e705f44e6b2ab28ec7fde5700589e46fa3e6385091720f01289d"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
