{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic python and ML Libraries\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# for ignoring warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# We will be reading images using OpenCV\n",
    "import cv2\n",
    "\n",
    "# xml library for parsing xml files\n",
    "from xml.etree import ElementTree as et\n",
    "\n",
    "# matplotlib for visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "# torchvision libraries\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision import transforms as torchtrans  \n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "\n",
    "# these are the helper libraries imported.\n",
    "from engine import train_one_epoch, evaluate\n",
    "import utils\n",
    "import transforms as T\n",
    "\n",
    "# for image augmentations\n",
    "import albumentations as A\n",
    "from albumentations.pytorch.transforms import ToTensorV2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IELDatasetTest(torch.utils.data.Dataset):\n",
    "\n",
    "    def __init__(self,images_dir, width, height, transforms=None):\n",
    "        self.transforms = transforms\n",
    "        self.images_dir = images_dir\n",
    "        self.label_dir = images_dir.replace(\"images\",\"labels\")\n",
    "        self.height = height\n",
    "        self.width = width\n",
    "        \n",
    "        # sorting the images for consistency\n",
    "        # To get images, the extension of the filename is checked to be jpg\n",
    "        self.imgs = [image for image in sorted(os.listdir(images_dir))\n",
    "                        if image[-4:]=='.jpg']\n",
    "        \n",
    "        for text_file in sorted(os.listdir(self.label_dir)):\n",
    "            l = 0\n",
    "            with open(os.path.join(self.label_dir,text_file),'r') as f:\n",
    "                for x in f:\n",
    "                    l += 1\n",
    "                    \n",
    "            if l == 0:\n",
    "                self.imgs.remove(text_file.replace('.txt','.jpg'))\n",
    "        \n",
    "        # classes: 0 index is reserved for background\n",
    "#         self.classes = [_, 'Epithelial Nuclei','IEL']\n",
    "        # self.classes = ['background', 'Inflammatory','Epithelial','Spindle']\n",
    "        self.classes = ['background', 'Epithelial','Lymphocyte','Neutrophil','Macrophage']\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        img_name = self.imgs[idx]\n",
    "        image_path = os.path.join(self.images_dir, img_name)\n",
    "\n",
    "        # reading the images and converting them to correct size and color    \n",
    "        img = cv2.imread(image_path)\n",
    "        img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB).astype(np.float32)\n",
    "        img_res = cv2.resize(img_rgb, (self.width, self.height), cv2.INTER_CUBIC)\n",
    "        # diving by 255\n",
    "        img_res /= 255.0\n",
    "        \n",
    "        boxes = []\n",
    "        labels = []\n",
    "        \n",
    "        wt = img.shape[1]\n",
    "        ht = img.shape[0]\n",
    "        \n",
    "        label_path = os.path.join(self.label_dir, img_name.replace('.jpg','.txt'))\n",
    "        \n",
    "        with open(label_path,'r') as f:\n",
    "            for line in f:\n",
    "                splits = line.split(' ')\n",
    "                w = float(splits[3]) * wt\n",
    "                h = float(splits[4]) * ht\n",
    "                x1 = ((2 * float(splits[1]) * wt) - w)/2\n",
    "                y1 = ((2 * float(splits[2]) * ht) - h)/2\n",
    "                x2 = x1 + w\n",
    "                y2 = y1 + h\n",
    "                \n",
    "                x1 = max(0,(x1/wt)*self.width)\n",
    "                x2 = min(self.width-1,(x2/wt)*self.width)\n",
    "                y1 = max(0,(y1/ht)*self.height)\n",
    "                y2 = min(self.height-1,(y2/ht)*self.height)\n",
    "                \n",
    "                if x1 >= x2 or y1 >= y2:\n",
    "                    continue\n",
    "                \n",
    "                boxes.append([x1,y1,x2,y2])\n",
    "                labels.append(int(splits[0]) + 1)\n",
    "                \n",
    "        boxes = [box for box in boxes if len(box) == 4]\n",
    "                \n",
    "        # convert boxes into a torch.Tensor\n",
    "        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
    "        \n",
    "        # getting the areas of the boxes\n",
    "        if boxes.shape[0] == 0:\n",
    "            area = boxes\n",
    "        else:\n",
    "            area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n",
    "\n",
    "        # suppose all instances are not crowd\n",
    "        iscrowd = torch.zeros((boxes.shape[0],), dtype=torch.int64)\n",
    "        \n",
    "        labels = torch.as_tensor(labels, dtype=torch.int64)\n",
    "        \n",
    "        target = {}\n",
    "        target[\"boxes\"] = boxes\n",
    "        target[\"labels\"] = labels\n",
    "        target[\"area\"] = area\n",
    "        target[\"iscrowd\"] = iscrowd\n",
    "        # image_id\n",
    "        image_id = torch.tensor([idx])\n",
    "        target[\"image_id\"] = image_id\n",
    "        target[\"image_name\"] = img_name\n",
    "\n",
    "\n",
    "        if self.transforms:\n",
    "            \n",
    "            sample = self.transforms(image = img_res,\n",
    "                                     bboxes = target['boxes'],\n",
    "                                     labels = labels)\n",
    "            \n",
    "            img_res = sample['image']\n",
    "            target['boxes'] = torch.Tensor(sample['bboxes'])\n",
    "            \n",
    "        return img_res , target\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the function takes the original prediction and the iou threshold.\n",
    "\n",
    "def apply_nms(orig_prediction, iou_thresh=0.3):\n",
    "    \n",
    "    # torchvision returns the indices of the bboxes to keep\n",
    "    keep = torchvision.ops.nms(orig_prediction['boxes'], orig_prediction['scores'], iou_thresh)\n",
    "    \n",
    "    final_prediction = orig_prediction\n",
    "    final_prediction['boxes'] = final_prediction['boxes'][keep]\n",
    "    final_prediction['scores'] = final_prediction['scores'][keep]\n",
    "    final_prediction['labels'] = final_prediction['labels'][keep]\n",
    "    \n",
    "    return final_prediction\n",
    "\n",
    "# function to convert a torchtensor back to PIL image\n",
    "def torch_to_pil(img):\n",
    "    return torchtrans.ToPILImage()(img).convert('RGB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to train on gpu if selected.\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "num_classes = 4\n",
    "\n",
    "# get the model using our helper function\n",
    "#model = get_object_detection_model(num_classes)\n",
    "# model = torch.load(\"/home/aayush/chirag/faster_rcnn/weights/faster_rcnn_MoNuSac_inc_dec_0001_Shape/faster_rcnn_49fasterrcnn_embed_monusac_shape_inc_balance_mean_embed_od0001_size_intensity_gmm_ep300.pt\")\n",
    "# model = torch.load(\"/home/aayush/chirag/faster_rcnn/weights/Celiac_kfold_weights/faster_rcnn_fasterrcnn_embed_kfold2_shape_inc_balance_mean_embed_size_intensity_gmm_combined.pt\")\n",
    "# model = torch.load(\"/home/aayush/chirag/faster_rcnn/weights/ConSep/faster-rcnn-consep.pt\")\n",
    "model = torch.load(\"/home/aayush/chirag/faster_rcnn/weights/faster_rcnn_ConSep/faster_rcnn_299fasterrcnn_embed_consep_shape_inc_balance_mean_embed_size_intensity_gmm_ep500.pt\")\n",
    "# model = torch.load(\"/home/aayush/chirag/faster_rcnn_monusac/faster_rcnn/weights/faster-rcnn-monusac_lr_005_ep100.pt\")\n",
    "\n",
    "\n",
    "# move model to the right device\n",
    "model.to(device)\n",
    "\n",
    "# construct an optimizer\n",
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer = torch.optim.SGD(params, lr=0.005,\n",
    "                            momentum=0.9, weight_decay=0.0005)\n",
    "\n",
    "# and a learning rate scheduler which decreases the learning rate by\n",
    "# 10x every 3 epochs\n",
    "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer,\n",
    "                                               step_size=3,\n",
    "                                               gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Send train=True fro training transforms and False for val/test transforms\n",
    "def get_transform(train):\n",
    "    \n",
    "    if train:\n",
    "        return A.Compose([\n",
    "                            A.HorizontalFlip(0.5),\n",
    "                     # ToTensorV2 converts image to pytorch tensor without div by 255\n",
    "                            ToTensorV2(p=1.0) \n",
    "                        ], bbox_params={'format': 'pascal_voc', 'label_fields': ['labels']})\n",
    "    else:\n",
    "        return A.Compose([\n",
    "                            ToTensorV2(p=1.0)\n",
    "                        ], bbox_params={'format': 'pascal_voc', 'label_fields': ['labels']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FasterRCNN(\n",
       "  (transform): GeneralizedRCNNTransform(\n",
       "      Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
       "      Resize(min_size=(800,), max_size=1333, mode='bilinear')\n",
       "  )\n",
       "  (backbone): BackboneWithFPN(\n",
       "    (body): IntermediateLayerGetter(\n",
       "      (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "      (bn1): FrozenBatchNorm2d(64, eps=0.0)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "      (layer1): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(64, eps=0.0)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(64, eps=0.0)\n",
       "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(64, eps=0.0)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(64, eps=0.0)\n",
       "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(64, eps=0.0)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(64, eps=0.0)\n",
       "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (layer2): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(128, eps=0.0)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(128, eps=0.0)\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(128, eps=0.0)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(128, eps=0.0)\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(128, eps=0.0)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(128, eps=0.0)\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (3): Bottleneck(\n",
       "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(128, eps=0.0)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(128, eps=0.0)\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (layer3): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): FrozenBatchNorm2d(1024, eps=0.0)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (3): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (4): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (5): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (layer4): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(2048, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): FrozenBatchNorm2d(2048, eps=0.0)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(2048, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(2048, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (fpn): FeaturePyramidNetwork(\n",
       "      (inner_blocks): ModuleList(\n",
       "        (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (2): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (3): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "      )\n",
       "      (layer_blocks): ModuleList(\n",
       "        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      )\n",
       "      (extra_blocks): LastLevelMaxPool()\n",
       "    )\n",
       "  )\n",
       "  (rpn): RegionProposalNetwork(\n",
       "    (anchor_generator): AnchorGenerator()\n",
       "    (head): RPNHead(\n",
       "      (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (cls_logits): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (bbox_pred): Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1))\n",
       "    )\n",
       "  )\n",
       "  (roi_heads): RoIHeads(\n",
       "    (box_roi_pool): MultiScaleRoIAlign(featmap_names=['0', '1', '2', '3'], output_size=(7, 7), sampling_ratio=2)\n",
       "    (box_head): TwoMLPHead(\n",
       "      (fc6): Linear(in_features=12544, out_features=1024, bias=True)\n",
       "      (fc7): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "    )\n",
       "    (box_predictor): FastRCNNPredictor(\n",
       "      (cls_score): Linear(in_features=1024, out_features=5, bias=True)\n",
       "      (bbox_pred): Linear(in_features=1024, out_features=20, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pick one image from the test set\n",
    "import numpy as np\n",
    "# fold_dir = '/home/aayush/chirag/tensorflow/yolo_kfold/2/images/'\n",
    "fold_dir = '/home/aayush/chirag/open_src_datasets/consep/yolo_format/images/'\n",
    "# fold_dir = '/home/aayush/chirag/open_src_datasets/monusac/yolo_format/images/'\n",
    "image_size = 500\n",
    "\n",
    "dataset_val = IELDatasetTest(os.path.join(fold_dir,\"test\"), image_size, image_size, transforms= get_transform(train=False))\n",
    "dataset_test = IELDatasetTest(os.path.join(fold_dir,\"test\"), image_size, image_size, transforms= get_transform(train=False))\n",
    "\n",
    "# put the model in evaluation mode\n",
    "model.eval()\n",
    "\n",
    "\n",
    "# for iel_thres in [0.1,0.3,0.5]:\n",
    "#     for epith_thres in [0.1,0.3,0.5]:\n",
    "#         print(iel_thres , epith_thres)\n",
    "#         print('---------------------------------------------')\n",
    "        \n",
    "#         target_dict = {}\n",
    "#         pred_dict = {}\n",
    "\n",
    "#         for i in range(dataset_test.__len__()):\n",
    "#             img,target = dataset_test[i]\n",
    "#             image_name = target[\"image_name\"][:-6]\n",
    "\n",
    "#             with torch.no_grad():\n",
    "#                 prediction = model([img.to(device)])[0]\n",
    "\n",
    "#             nms_prediction = apply_nms(prediction, iou_thresh=0.01)\n",
    "\n",
    "#             target_labels = target[\"labels\"].numpy()\n",
    "#             pred_labels = list(nms_prediction[\"labels\"].cpu())\n",
    "#             pred_scores = list(nms_prediction[\"scores\"].cpu())\n",
    "#             num_iels = 0\n",
    "#             num_epith = 0\n",
    "\n",
    "#             for i in range(len(pred_labels)):\n",
    "#                 if pred_labels[i] == 1:\n",
    "#                     if pred_scores[i] >= epith_thres:\n",
    "#                         num_epith += 1\n",
    "#                 else:\n",
    "#                     if pred_scores[i] >= iel_thres:\n",
    "#                         num_iels += 1\n",
    "\n",
    "#             if image_name in target_dict:\n",
    "#                 target_dict[image_name][\"iel\"] += np.count_nonzero(target_labels == 2)\n",
    "#                 target_dict[image_name][\"epith\"] += np.count_nonzero(target_labels == 1)\n",
    "#                 pred_dict[image_name][\"iel\"] += num_iels\n",
    "#                 pred_dict[image_name][\"epith\"] += num_epith\n",
    "\n",
    "#             else:\n",
    "#                 target_dict[image_name] = {}\n",
    "#                 target_dict[image_name][\"iel\"] = np.count_nonzero(target_labels == 2)\n",
    "#                 target_dict[image_name][\"epith\"] = np.count_nonzero(target_labels == 1)\n",
    "\n",
    "#                 pred_dict[image_name] = {}\n",
    "#                 pred_dict[image_name][\"iel\"] = num_iels\n",
    "#                 pred_dict[image_name][\"epith\"] = num_epith\n",
    "\n",
    "#         mae_iel = 0\n",
    "#         mre_iel = 0\n",
    "#         mae_epith = 0\n",
    "#         mre_epith = 0\n",
    "#         num_images = 0\n",
    "#         mae_ratio = 0\n",
    "#         mre_ratio = 0\n",
    "\n",
    "#         for key in target_dict.keys():\n",
    "#             num_images += 1\n",
    "#             #print(key , target_dict[key][\"iel\"] , pred_dict[key][\"iel\"], target_dict[key][\"epith\"], pred_dict[key][\"epith\"] , target_dict[key][\"iel\"]/target_dict[key][\"epith\"])\n",
    "#             mae_iel += abs(target_dict[key][\"iel\"] - pred_dict[key][\"iel\"])\n",
    "#             mre_iel += abs(target_dict[key][\"iel\"] - pred_dict[key][\"iel\"])/target_dict[key][\"iel\"]\n",
    "#             mae_epith += abs(target_dict[key][\"epith\"] - pred_dict[key][\"epith\"])\n",
    "#             mre_epith += abs(target_dict[key][\"epith\"] - pred_dict[key][\"epith\"])/target_dict[key][\"epith\"]\n",
    "            \n",
    "#             if pred_dict[key][\"epith\"] != 0:\n",
    "#                 print(key, (100*target_dict[key][\"iel\"])/target_dict[key][\"epith\"], (100*pred_dict[key][\"iel\"])/pred_dict[key][\"epith\"], target_dict[key][\"iel\"], pred_dict[key][\"iel\"], target_dict[key][\"epith\"], pred_dict[key][\"epith\"])\n",
    "#                 mae_ratio += abs((100*target_dict[key][\"iel\"])/target_dict[key][\"epith\"] - (100*pred_dict[key][\"iel\"])/pred_dict[key][\"epith\"])\n",
    "                \n",
    "#                 if target_dict[key][\"iel\"] != 0:\n",
    "#                     mre_ratio += ((abs((100*target_dict[key][\"iel\"])/target_dict[key][\"epith\"] - (100*pred_dict[key][\"iel\"])/pred_dict[key][\"epith\"])) / ((100*target_dict[key][\"iel\"])/target_dict[key][\"epith\"]))\n",
    "#             else:\n",
    "#                 print('Not nice :' ,key, target_dict[key][\"iel\"], pred_dict[key][\"iel\"], target_dict[key][\"epith\"], pred_dict[key][\"epith\"])\n",
    "\n",
    "#         mae_iel /= num_images\n",
    "#         mre_iel /= num_images\n",
    "#         mae_epith /= num_images\n",
    "#         mre_epith /= num_images\n",
    "#         mae_ratio /= num_images\n",
    "#         mre_ratio /= num_images\n",
    "        \n",
    "        \n",
    "#         print(mae_iel , mre_iel , mae_epith , mre_epith , mae_ratio, mre_ratio)\n",
    "#         print('----------------------------------------')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_10 297 239 338 84 209 22\n",
      "test_11 7 106 618 149 9 68\n",
      "test_12 102 72 166 101 159 104\n",
      "test_13 595 292 0 21 757 57\n",
      "test_14 11 56 483 196 56 55\n",
      "test_1 71 121 214 71 578 135\n",
      "test_2 15 7 0 13 229 186\n",
      "test_3 2 13 265 142 103 104\n",
      "test_4 5 25 302 166 59 103\n",
      "test_5 192 175 212 91 479 98\n",
      "test_6 111 58 0 14 186 155\n",
      "test_7 63 44 0 33 172 136\n",
      "test_8 2 8 609 193 10 60\n",
      "test_9 162 154 0 23 350 99\n",
      "51.92857142857143 151.28571428571428 163.0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "conf0, conf1, conf2 = 0.1, 0, 0.1\n",
    "\n",
    "target_dict = {}\n",
    "pred_dict = {}\n",
    "\n",
    "for i in range(dataset_test.__len__()):\n",
    "    img,target = dataset_test[i]\n",
    "    image_name = target[\"image_name\"][:-6]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        prediction = model([img.to(device)])[0]\n",
    "\n",
    "    nms_prediction = apply_nms(prediction, iou_thresh=0.01)\n",
    "\n",
    "    target_labels = target[\"labels\"].numpy()\n",
    "    pred_labels = list(nms_prediction[\"labels\"].cpu())\n",
    "    pred_scores = list(nms_prediction[\"scores\"].cpu())\n",
    "    num_0 = 0\n",
    "    num_1 = 0\n",
    "    num_2 = 0\n",
    "\n",
    "    for i in range(len(pred_labels)):\n",
    "        if pred_labels[i] == 1 and pred_scores[i] >= conf0:\n",
    "            num_0 += 1\n",
    "        if pred_labels[i] == 2 and pred_scores[i] >= conf1:\n",
    "            num_1 += 1\n",
    "        if pred_labels[i] == 3 and pred_scores[i] >= conf2:\n",
    "            num_2 += 1\n",
    "\n",
    "    if image_name in target_dict:\n",
    "        target_dict[image_name][1] += np.count_nonzero(target_labels == 1)\n",
    "        target_dict[image_name][2] += np.count_nonzero(target_labels == 2)\n",
    "        target_dict[image_name][3] += np.count_nonzero(target_labels == 3)\n",
    "        pred_dict[image_name][1] += num_0\n",
    "        pred_dict[image_name][2] += num_1\n",
    "        pred_dict[image_name][3] += num_2\n",
    "        \n",
    "\n",
    "    else:\n",
    "        target_dict[image_name] = {}\n",
    "        target_dict[image_name][1] = np.count_nonzero(target_labels == 1)\n",
    "        target_dict[image_name][2] = np.count_nonzero(target_labels == 2)\n",
    "        target_dict[image_name][3] = np.count_nonzero(target_labels == 3)\n",
    "        pred_dict[image_name] = {}\n",
    "        pred_dict[image_name][1] = num_0\n",
    "        pred_dict[image_name][2] = num_1\n",
    "        pred_dict[image_name][3] = num_2\n",
    "\n",
    "mae_1 = 0\n",
    "mae_2 = 0\n",
    "mae_3 = 0\n",
    "num_images = 0\n",
    "\n",
    "for key in target_dict.keys():\n",
    "    num_images += 1\n",
    "    #print(key , target_dict[key][\"iel\"] , pred_dict[key][\"iel\"], target_dict[key][\"epith\"], pred_dict[key][\"epith\"] , target_dict[key][\"iel\"]/target_dict[key][\"epith\"])\n",
    "    print(key, target_dict[key][1], pred_dict[key][1], target_dict[key][2], pred_dict[key][2], target_dict[key][3], pred_dict[key][3] )\n",
    "    mae_1 += abs(target_dict[key][1] - pred_dict[key][1])\n",
    "    mae_2 += abs(target_dict[key][2] - pred_dict[key][2])\n",
    "    mae_3 += abs(target_dict[key][3] - pred_dict[key][3])\n",
    "    \n",
    "\n",
    "mae_1 /= num_images\n",
    "mae_2 /= num_images\n",
    "mae_3 /= num_images\n",
    "\n",
    "print(mae_1, mae_2, mae_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_10 297 239 338 84 209 22 0 0\n",
      "test_11 7 106 618 149 9 68 0 0\n",
      "test_12 102 72 166 101 159 104 0 0\n",
      "test_13 595 292 0 21 757 57 0 0\n",
      "test_14 11 56 483 196 56 55 0 0\n",
      "test_1 71 121 214 71 578 135 0 0\n",
      "test_2 15 7 0 13 229 186 0 0\n",
      "test_3 2 13 265 142 103 104 0 0\n",
      "test_4 5 25 302 166 59 103 0 0\n",
      "test_5 192 175 212 91 479 98 0 0\n",
      "test_6 111 58 0 14 186 155 0 0\n",
      "test_7 63 44 0 33 172 136 0 0\n",
      "test_8 2 8 609 193 10 60 0 0\n",
      "test_9 162 154 0 23 350 99 0 0\n",
      "51.92857142857143 151.28571428571428 163.0 0.0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "conf0, conf1, conf2, conf3 = 0.1, 0.1, 0.1, 0.1\n",
    "\n",
    "target_dict = {}\n",
    "pred_dict = {}\n",
    "\n",
    "for i in range(dataset_test.__len__()):\n",
    "    img,target = dataset_test[i]\n",
    "    image_name = target[\"image_name\"][:-6]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        prediction = model([img.to(device)])[0]\n",
    "\n",
    "    nms_prediction = apply_nms(prediction, iou_thresh=0.01)\n",
    "\n",
    "    target_labels = target[\"labels\"].numpy()\n",
    "    pred_labels = list(nms_prediction[\"labels\"].cpu())\n",
    "    pred_scores = list(nms_prediction[\"scores\"].cpu())\n",
    "    num_0 = 0\n",
    "    num_1 = 0\n",
    "    num_2 = 0\n",
    "    num_3 = 0\n",
    "\n",
    "    for i in range(len(pred_labels)):\n",
    "        if pred_labels[i] == 1 and pred_scores[i] >= conf0:\n",
    "            num_0 += 1\n",
    "        if pred_labels[i] == 2 and pred_scores[i] >= conf1:\n",
    "            num_1 += 1\n",
    "        if pred_labels[i] == 3 and pred_scores[i] >= conf2:\n",
    "            num_2 += 1\n",
    "        if pred_labels[i] == 4 and pred_scores[i] >= conf3:\n",
    "            num_3 += 1\n",
    "\n",
    "    if image_name in target_dict:\n",
    "        target_dict[image_name][1] += np.count_nonzero(target_labels == 1)\n",
    "        target_dict[image_name][2] += np.count_nonzero(target_labels == 2)\n",
    "        target_dict[image_name][3] += np.count_nonzero(target_labels == 3)\n",
    "        target_dict[image_name][4] += np.count_nonzero(target_labels == 4)\n",
    "        pred_dict[image_name][1] += num_0\n",
    "        pred_dict[image_name][2] += num_1\n",
    "        pred_dict[image_name][3] += num_2\n",
    "        pred_dict[image_name][4] += num_3\n",
    "        \n",
    "\n",
    "    else:\n",
    "        target_dict[image_name] = {}\n",
    "        target_dict[image_name][1] = np.count_nonzero(target_labels == 1)\n",
    "        target_dict[image_name][2] = np.count_nonzero(target_labels == 2)\n",
    "        target_dict[image_name][3] = np.count_nonzero(target_labels == 3)\n",
    "        target_dict[image_name][4] = np.count_nonzero(target_labels == 4)\n",
    "        pred_dict[image_name] = {}\n",
    "        pred_dict[image_name][1] = num_0\n",
    "        pred_dict[image_name][2] = num_1\n",
    "        pred_dict[image_name][3] = num_2\n",
    "        pred_dict[image_name][4] = num_3\n",
    "\n",
    "mae_1 = 0\n",
    "mae_2 = 0\n",
    "mae_3 = 0\n",
    "mae_4 = 0\n",
    "num_images = 0\n",
    "\n",
    "for key in target_dict.keys():\n",
    "    num_images += 1\n",
    "    #print(key , target_dict[key][\"iel\"] , pred_dict[key][\"iel\"], target_dict[key][\"epith\"], pred_dict[key][\"epith\"] , target_dict[key][\"iel\"]/target_dict[key][\"epith\"])\n",
    "    print(key, target_dict[key][1], pred_dict[key][1], target_dict[key][2], pred_dict[key][2], target_dict[key][3], pred_dict[key][3], target_dict[key][4], pred_dict[key][4] )\n",
    "    mae_1 += abs(target_dict[key][1] - pred_dict[key][1])\n",
    "    mae_2 += abs(target_dict[key][2] - pred_dict[key][2])\n",
    "    mae_3 += abs(target_dict[key][3] - pred_dict[key][3])\n",
    "    mae_4 += abs(target_dict[key][4] - pred_dict[key][4])\n",
    "    \n",
    "\n",
    "mae_1 /= num_images\n",
    "mae_2 /= num_images\n",
    "mae_3 /= num_images\n",
    "mae_4 /= num_images\n",
    "\n",
    "print(mae_1, mae_2, mae_3, mae_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def box_iou(box1, box2):\n",
    "    # https://github.com/pytorch/vision/blob/master/torchvision/ops/boxes.py\n",
    "    \"\"\"\n",
    "    Return intersection-over-union (Jaccard index) of boxes.\n",
    "    Both sets of boxes are expected to be in (x1, y1, x2, y2) format.\n",
    "    Arguments:\n",
    "        box1 (Tensor[N, 4])\n",
    "        box2 (Tensor[M, 4])\n",
    "    Returns:\n",
    "        iou (Tensor[N, M]): the NxM matrix containing the pairwise\n",
    "            IoU values for every element in boxes1 and boxes2\n",
    "    \"\"\"\n",
    "\n",
    "    def box_area(box):\n",
    "        # box = 4xn\n",
    "        return (box[2] - box[0]) * (box[3] - box[1])\n",
    "\n",
    "    area1 = box_area(box1.T)\n",
    "    area2 = box_area(box2.T)\n",
    "\n",
    "    # inter(N,M) = (rb(N,M,2) - lt(N,M,2)).clamp(0).prod(2)\n",
    "    inter = (torch.min(box1[:, None, 2:], box2[:, 2:]) - torch.max(box1[:, None, :2], box2[:, :2])).clamp(0).prod(2)\n",
    "    return inter / (area1[:, None] + area2 - inter)  # iou = inter / (area1 + area2 - inter)\n",
    "\n",
    "def process_batch(detections, labels, iouv):\n",
    "    \"\"\"\n",
    "    Return correct predictions matrix. Both sets of boxes are in (x1, y1, x2, y2) format.\n",
    "    Arguments:\n",
    "        detections (Array[N, 6]), x1, y1, x2, y2, conf, class\n",
    "        labels (Array[M, 5]), class, x1, y1, x2, y2\n",
    "    Returns:\n",
    "        correct (Array[N, 10]), for 10 IoU levels\n",
    "    \"\"\"\n",
    "    correct = torch.zeros(detections.shape[0], iouv.shape[0], dtype=torch.bool, device=iouv.device)\n",
    "    iou = box_iou(labels[:, 1:], detections[:, :4])\n",
    "    x = torch.where((iou >= iouv[0]) & (labels[:, 0:1] == detections[:, 5]))  # IoU above threshold and classes match\n",
    "    if x[0].shape[0]:\n",
    "        matches = torch.cat((torch.stack(x, 1), iou[x[0], x[1]][:, None]), 1).cpu().numpy()  # [label, detection, iou]\n",
    "        if x[0].shape[0] > 1:\n",
    "            matches = matches[matches[:, 2].argsort()[::-1]]\n",
    "            matches = matches[np.unique(matches[:, 1], return_index=True)[1]]\n",
    "            # matches = matches[matches[:, 2].argsort()[::-1]]\n",
    "            matches = matches[np.unique(matches[:, 0], return_index=True)[1]]\n",
    "        matches = torch.Tensor(matches).to(iouv.device)\n",
    "        correct[matches[:, 1].long()] = matches[:, 2:3] >= iouv\n",
    "    return correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_ap(recall, precision):\n",
    "    \"\"\" Compute the average precision, given the recall and precision curves\n",
    "    # Arguments\n",
    "        recall:    The recall curve (list)\n",
    "        precision: The precision curve (list)\n",
    "    # Returns\n",
    "        Average precision, precision curve, recall curve\n",
    "    \"\"\"\n",
    "\n",
    "    # Append sentinel values to beginning and end\n",
    "    mrec = np.concatenate(([0.0], recall, [1.0]))\n",
    "    mpre = np.concatenate(([1.0], precision, [0.0]))\n",
    "\n",
    "    # Compute the precision envelope\n",
    "    mpre = np.flip(np.maximum.accumulate(np.flip(mpre)))\n",
    "\n",
    "    # Integrate area under curve\n",
    "    method = 'interp'  # methods: 'continuous', 'interp'\n",
    "    if method == 'interp':\n",
    "        x = np.linspace(0, 1, 101)  # 101-point interp (COCO)\n",
    "        ap = np.trapz(np.interp(x, mrec, mpre), x)  # integrate\n",
    "    else:  # 'continuous'\n",
    "        i = np.where(mrec[1:] != mrec[:-1])[0]  # points where x axis (recall) changes\n",
    "        ap = np.sum((mrec[i + 1] - mrec[i]) * mpre[i + 1])  # area under curve\n",
    "\n",
    "    return ap, mpre, mrec\n",
    "\n",
    "def ap_per_class(tp, conf, pred_cls, target_cls, plot=False, save_dir='.', names=(), eps=1e-16):\n",
    "    \"\"\" Compute the average precision, given the recall and precision curves.\n",
    "    Source: https://github.com/rafaelpadilla/Object-Detection-Metrics.\n",
    "    # Arguments\n",
    "        tp:  True positives (nparray, nx1 or nx10).\n",
    "        conf:  Objectness value from 0-1 (nparray).\n",
    "        pred_cls:  Predicted object classes (nparray).\n",
    "        target_cls:  True object classes (nparray).\n",
    "        plot:  Plot precision-recall curve at mAP@0.5\n",
    "        save_dir:  Plot save directory\n",
    "    # Returns\n",
    "        The average precision as computed in py-faster-rcnn.\n",
    "    \"\"\"\n",
    "\n",
    "    # Sort by objectness\n",
    "    i = np.argsort(-conf)\n",
    "    tp, conf, pred_cls = tp[i], conf[i], pred_cls[i]\n",
    "\n",
    "    # Find unique classes\n",
    "    unique_classes, nt = np.unique(target_cls, return_counts=True)\n",
    "    nc = unique_classes.shape[0]  # number of classes, number of detections\n",
    "\n",
    "    # Create Precision-Recall curve and compute AP for each class\n",
    "    px, py = np.linspace(0, 1, 1000), []  # for plotting\n",
    "    ap, p, r = np.zeros((nc, tp.shape[1])), np.zeros((nc, 1000)), np.zeros((nc, 1000))\n",
    "    for ci, c in enumerate(unique_classes):\n",
    "        i = pred_cls == c\n",
    "        n_l = nt[ci]  # number of labels\n",
    "        n_p = i.sum()  # number of predictions\n",
    "\n",
    "        if n_p == 0 or n_l == 0:\n",
    "            continue\n",
    "        else:\n",
    "            # Accumulate FPs and TPs\n",
    "            fpc = (1 - tp[i]).cumsum(0)\n",
    "            tpc = tp[i].cumsum(0)\n",
    "\n",
    "            # Recall\n",
    "            recall = tpc / (n_l + eps)  # recall curve\n",
    "            r[ci] = np.interp(-px, -conf[i], recall[:, 0], left=0)  # negative x, xp because xp decreases\n",
    "\n",
    "            # Precision\n",
    "            precision = tpc / (tpc + fpc)  # precision curve\n",
    "            p[ci] = np.interp(-px, -conf[i], precision[:, 0], left=1)  # p at pr_score\n",
    "\n",
    "            # AP from recall-precision curve\n",
    "            for j in range(tp.shape[1]):\n",
    "                ap[ci, j], mpre, mrec = compute_ap(recall[:, j], precision[:, j])\n",
    "                if plot and j == 0:\n",
    "                    py.append(np.interp(px, mrec, mpre))  # precision at mAP@0.5\n",
    "\n",
    "    # Compute F1 (harmonic mean of precision and recall)\n",
    "    f1 = 2 * p * r / (p + r + eps)\n",
    "\n",
    "    i = f1.mean(0).argmax()  # max F1 index\n",
    "    p, r, f1 = p[:, i], r[:, i], f1[:, i]\n",
    "    tp = (r * nt).round()  # true positives\n",
    "    fp = (tp / (p + eps) - tp).round()  # false positives\n",
    "    return tp, fp, p, r, f1, ap, unique_classes.astype('int32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5716106173277811 0.33113696145851645 0.45190920243186206 0.2202741202114309\n"
     ]
    }
   ],
   "source": [
    "# pick one image from the test set\n",
    "import numpy as np\n",
    "\n",
    "dataset_val = IELDatasetTest(os.path.join(fold_dir,\"test\"), image_size, image_size, transforms= get_transform(train=False))\n",
    "dataset_test = IELDatasetTest(os.path.join(fold_dir,\"test\"), image_size, image_size, transforms= get_transform(train=False))\n",
    "\n",
    "# put the model in evaluation mode\n",
    "model.eval()\n",
    "\n",
    "mAPl = 0.5\n",
    "mAPr = 0.95\n",
    "iouv = torch.linspace(mAPl, mAPr, 10).to(device)  # iou vector for mAP@0.5:0.\n",
    "niou = iouv.numel()\n",
    "\n",
    "jdict, stats, ap, ap_class = [], [], [], []\n",
    "\n",
    "for i in range(dataset_test.__len__()):\n",
    "    img,target = dataset_test[i]\n",
    "    image_name = target[\"image_name\"][:-6]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        prediction = model([img.to(device)])[0]\n",
    "\n",
    "    nms_prediction = apply_nms(prediction, iou_thresh=0.01)\n",
    "    \n",
    "    labelsn = torch.cat((torch.reshape(target[\"labels\"],(target[\"labels\"].shape[0],1)), target[\"boxes\"]), 1)\n",
    "    labelsn = labelsn.to(device)\n",
    "    \n",
    "    pred_boxes = nms_prediction[\"boxes\"]\n",
    "    pred_labels = nms_prediction[\"labels\"]\n",
    "    pred_conf = nms_prediction[\"scores\"]\n",
    "    \n",
    "    predn = torch.cat((pred_boxes, torch.reshape(pred_conf , (pred_conf.shape[0],1)) , torch.reshape(pred_labels, (pred_labels.shape[0],1))), 1)\n",
    "    \n",
    "    # print(labelsn.shape,predn.shape)\n",
    "    \n",
    "    correct = process_batch(predn, labelsn, iouv)\n",
    "    \n",
    "    stats.append((correct.cpu(), predn[:, 4].cpu(), predn[:, 5].cpu(), target[\"labels\"].tolist()))  # (correct, conf, pcls, tcls)\n",
    "\n",
    "stats = [np.concatenate(x, 0) for x in zip(*stats)]  # to numpy\n",
    "if len(stats) and stats[0].any():\n",
    "    tp, fp, p, r, f1, ap, ap_class = ap_per_class(*stats)\n",
    "    ap50, ap = ap[:, 0], ap.mean(1)  # AP@0.5, AP@0.5:0.95\n",
    "    mp, mr, map50, map5095 = p.mean(), r.mean(), ap50.mean(), ap.mean()\n",
    "    \n",
    "print(mp, mr, map50, map5095)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "fce2ee5fb604e705f44e6b2ab28ec7fde5700589e46fa3e6385091720f01289d"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
