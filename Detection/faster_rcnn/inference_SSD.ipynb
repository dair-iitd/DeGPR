{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic python and ML Libraries\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# for ignoring warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# We will be reading images using OpenCV\n",
    "import cv2\n",
    "\n",
    "# xml library for parsing xml files\n",
    "from xml.etree import ElementTree as et\n",
    "\n",
    "# matplotlib for visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "# torchvision libraries\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision import transforms as torchtrans  \n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "\n",
    "# these are the helper libraries imported.\n",
    "from engine import train_one_epoch, evaluate\n",
    "import utils\n",
    "import transforms as T\n",
    "\n",
    "# for image augmentations\n",
    "import albumentations as A\n",
    "from albumentations.pytorch.transforms import ToTensorV2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IELDatasetTest(torch.utils.data.Dataset):\n",
    "\n",
    "    def __init__(self,images_dir, width, height, transforms=None):\n",
    "        self.transforms = transforms\n",
    "        self.images_dir = images_dir\n",
    "        self.label_dir = images_dir.replace(\"images\",\"labels\")\n",
    "        self.height = height\n",
    "        self.width = width\n",
    "        \n",
    "        # sorting the images for consistency\n",
    "        # To get images, the extension of the filename is checked to be jpg\n",
    "        self.imgs = [image for image in sorted(os.listdir(images_dir))\n",
    "                        if image[-4:]=='.jpg']\n",
    "        \n",
    "        for text_file in sorted(os.listdir(self.label_dir)):\n",
    "            l = 0\n",
    "            with open(os.path.join(self.label_dir,text_file),'r') as f:\n",
    "                for x in f:\n",
    "                    l += 1\n",
    "                    \n",
    "            if l == 0:\n",
    "                self.imgs.remove(text_file.replace('.txt','.jpg'))\n",
    "        \n",
    "        # classes: 0 index is reserved for background\n",
    "        self.classes = [_, 'Epithelial Nuclei','IEL']\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        img_name = self.imgs[idx]\n",
    "        image_path = os.path.join(self.images_dir, img_name)\n",
    "\n",
    "        # reading the images and converting them to correct size and color    \n",
    "        img = cv2.imread(image_path)\n",
    "        img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB).astype(np.float32)\n",
    "        img_res = cv2.resize(img_rgb, (self.width, self.height), cv2.INTER_CUBIC)\n",
    "        # diving by 255\n",
    "        img_res /= 255.0\n",
    "        \n",
    "        boxes = []\n",
    "        labels = []\n",
    "        \n",
    "        wt = img.shape[1]\n",
    "        ht = img.shape[0]\n",
    "        \n",
    "        label_path = os.path.join(self.label_dir, img_name.replace('.jpg','.txt'))\n",
    "        \n",
    "        with open(label_path,'r') as f:\n",
    "            for line in f:\n",
    "                splits = line.split(' ')\n",
    "                w = float(splits[3]) * wt\n",
    "                h = float(splits[4]) * ht\n",
    "                x1 = ((2 * float(splits[1]) * wt) - w)/2\n",
    "                y1 = ((2 * float(splits[2]) * ht) - h)/2\n",
    "                x2 = x1 + w\n",
    "                y2 = y1 + h\n",
    "                \n",
    "                x1 = max(0,(x1/wt)*self.width)\n",
    "                x2 = min(self.width-1,(x2/wt)*self.width)\n",
    "                y1 = max(0,(y1/ht)*self.height)\n",
    "                y2 = min(self.height-1,(y2/ht)*self.height)\n",
    "                \n",
    "                if x1 >= x2 or y1 >= y2:\n",
    "                    continue\n",
    "                \n",
    "                boxes.append([x1,y1,x2,y2])\n",
    "                labels.append(int(splits[0]) + 1)\n",
    "                \n",
    "        boxes = [box for box in boxes if len(box) == 4]\n",
    "                \n",
    "        # convert boxes into a torch.Tensor\n",
    "        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
    "        \n",
    "        # getting the areas of the boxes\n",
    "        if boxes.shape[0] == 0:\n",
    "            area = boxes\n",
    "        else:\n",
    "            area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n",
    "\n",
    "        # suppose all instances are not crowd\n",
    "        iscrowd = torch.zeros((boxes.shape[0],), dtype=torch.int64)\n",
    "        \n",
    "        labels = torch.as_tensor(labels, dtype=torch.int64)\n",
    "        \n",
    "        target = {}\n",
    "        target[\"boxes\"] = boxes\n",
    "        target[\"labels\"] = labels\n",
    "        target[\"area\"] = area\n",
    "        target[\"iscrowd\"] = iscrowd\n",
    "        # image_id\n",
    "        image_id = torch.tensor([idx])\n",
    "        target[\"image_id\"] = image_id\n",
    "        target[\"image_name\"] = img_name\n",
    "\n",
    "\n",
    "        if self.transforms:\n",
    "            \n",
    "            sample = self.transforms(image = img_res,\n",
    "                                     bboxes = target['boxes'],\n",
    "                                     labels = labels)\n",
    "            \n",
    "            img_res = sample['image']\n",
    "            target['boxes'] = torch.Tensor(sample['bboxes'])\n",
    "            \n",
    "        return img_res , target\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the function takes the original prediction and the iou threshold.\n",
    "\n",
    "def apply_nms(orig_prediction, iou_thresh=0.3):\n",
    "    \n",
    "    # torchvision returns the indices of the bboxes to keep\n",
    "    keep = torchvision.ops.nms(orig_prediction['boxes'], orig_prediction['scores'], iou_thresh)\n",
    "    \n",
    "    final_prediction = orig_prediction\n",
    "    final_prediction['boxes'] = final_prediction['boxes'][keep]\n",
    "    final_prediction['scores'] = final_prediction['scores'][keep]\n",
    "    final_prediction['labels'] = final_prediction['labels'][keep]\n",
    "    \n",
    "    return final_prediction\n",
    "\n",
    "# function to convert a torchtensor back to PIL image\n",
    "def torch_to_pil(img):\n",
    "    return torchtrans.ToPILImage()(img).convert('RGB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'collections.OrderedDict' object has no attribute 'to'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1116803/3345151044.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# move model to the right device\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m# construct an optimizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'collections.OrderedDict' object has no attribute 'to'"
     ]
    }
   ],
   "source": [
    "# to train on gpu if selected.\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "num_classes = 3\n",
    "\n",
    "# get the model using our helper function\n",
    "#model = get_object_detection_model(num_classes)\n",
    "model = torch.load(\"/home/aayush/Aayush/Projects/Celiac_Disease/Detection/Baselines/SSD/Repo/ssd.pytorch/weights/COCO.pth\")\n",
    "\n",
    "# move model to the right device\n",
    "model.to(device)\n",
    "\n",
    "# construct an optimizer\n",
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer = torch.optim.SGD(params, lr=0.005,\n",
    "                            momentum=0.9, weight_decay=0.0005)\n",
    "\n",
    "# and a learning rate scheduler which decreases the learning rate by\n",
    "# 10x every 3 epochs\n",
    "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer,\n",
    "                                               step_size=3,\n",
    "                                               gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Send train=True fro training transforms and False for val/test transforms\n",
    "def get_transform(train):\n",
    "    \n",
    "    if train:\n",
    "        return A.Compose([\n",
    "                            A.HorizontalFlip(0.5),\n",
    "                     # ToTensorV2 converts image to pytorch tensor without div by 255\n",
    "                            ToTensorV2(p=1.0) \n",
    "                        ], bbox_params={'format': 'pascal_voc', 'label_fields': ['labels']})\n",
    "    else:\n",
    "        return A.Compose([\n",
    "                            ToTensorV2(p=1.0)\n",
    "                        ], bbox_params={'format': 'pascal_voc', 'label_fields': ['labels']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1 0.1\n",
      "---------------------------------------------\n",
      "Image_23978 53.225806451612904 54.54545454545455 66 48 124 88\n",
      "Image_23979 58.490566037735846 55.056179775280896 62 49 106 89\n",
      "Image_23980 34.51327433628319 58.53658536585366 39 48 113 82\n",
      "Image_23982 22.641509433962263 26.923076923076923 24 28 106 104\n",
      "Image_23986 4.25531914893617 10.81081081081081 4 8 94 74\n",
      "Image_23988 7.03125 10.227272727272727 9 9 128 88\n",
      "Image_23989 17.24137931034483 11.11111111111111 10 5 58 45\n",
      "Image_23990 4.166666666666667 7.894736842105263 2 3 48 38\n",
      "Image_23991 5.128205128205129 2.7777777777777777 2 1 39 36\n",
      "Image_23994 38.983050847457626 29.12621359223301 46 30 118 103\n",
      "Image_23995 15.714285714285714 22.727272727272727 11 15 70 66\n",
      "6.818181818181818 0.37193664001008125 17.363636363636363 0.17843872243620226 6.535365205221853 0.488319539799787\n",
      "----------------------------------------\n",
      "0.1 0.3\n",
      "---------------------------------------------\n",
      "Image_23978 53.225806451612904 55.172413793103445 66 48 124 87\n",
      "Image_23979 58.490566037735846 58.333333333333336 62 49 106 84\n",
      "Image_23980 34.51327433628319 60.75949367088607 39 48 113 79\n",
      "Image_23982 22.641509433962263 29.166666666666668 24 28 106 96\n",
      "Image_23986 4.25531914893617 12.307692307692308 4 8 94 65\n",
      "Image_23988 7.03125 11.39240506329114 9 9 128 79\n",
      "Image_23989 17.24137931034483 13.513513513513514 10 5 58 37\n",
      "Image_23990 4.166666666666667 9.375 2 3 48 32\n",
      "Image_23991 5.128205128205129 3.225806451612903 2 1 39 31\n",
      "Image_23994 38.983050847457626 30.612244897959183 46 30 118 98\n",
      "Image_23995 15.714285714285714 25.862068965517242 11 15 70 58\n",
      "6.818181818181818 0.37193664001008125 23.454545454545453 0.2576302349189262 6.9678119857031335 0.5725604390991925\n",
      "----------------------------------------\n",
      "0.1 0.5\n",
      "---------------------------------------------\n",
      "Image_23978 53.225806451612904 68.57142857142857 66 48 124 70\n",
      "Image_23979 58.490566037735846 71.01449275362319 62 49 106 69\n",
      "Image_23980 34.51327433628319 80.0 39 48 113 60\n",
      "Image_23982 22.641509433962263 36.36363636363637 24 28 106 77\n",
      "Image_23986 4.25531914893617 16.666666666666668 4 8 94 48\n",
      "Image_23988 7.03125 15.517241379310345 9 9 128 58\n",
      "Image_23989 17.24137931034483 15.625 10 5 58 32\n",
      "Image_23990 4.166666666666667 10.344827586206897 2 3 48 29\n",
      "Image_23991 5.128205128205129 3.8461538461538463 2 1 39 26\n",
      "Image_23994 38.983050847457626 37.5 46 30 118 80\n",
      "Image_23995 15.714285714285714 31.25 11 15 70 48\n",
      "6.818181818181818 0.37193664001008125 37.0 0.3979227975025783 12.188281542840274 0.854835585488798\n",
      "----------------------------------------\n",
      "0.3 0.1\n",
      "---------------------------------------------\n",
      "Image_23978 53.225806451612904 54.54545454545455 66 48 124 88\n",
      "Image_23979 58.490566037735846 50.561797752808985 62 45 106 89\n",
      "Image_23980 34.51327433628319 53.65853658536585 39 44 113 82\n",
      "Image_23982 22.641509433962263 25.96153846153846 24 27 106 104\n",
      "Image_23986 4.25531914893617 8.108108108108109 4 6 94 74\n",
      "Image_23988 7.03125 10.227272727272727 9 9 128 88\n",
      "Image_23989 17.24137931034483 11.11111111111111 10 5 58 45\n",
      "Image_23990 4.166666666666667 2.6315789473684212 2 1 48 38\n",
      "Image_23991 5.128205128205129 2.7777777777777777 2 1 39 36\n",
      "Image_23994 38.983050847457626 23.300970873786408 46 24 118 103\n",
      "Image_23995 15.714285714285714 13.636363636363637 11 9 70 66\n",
      "6.909090909090909 0.3145640909729906 17.363636363636363 0.17843872243620226 6.04893678749315 0.3580434286464337\n",
      "----------------------------------------\n",
      "0.3 0.3\n",
      "---------------------------------------------\n",
      "Image_23978 53.225806451612904 55.172413793103445 66 48 124 87\n",
      "Image_23979 58.490566037735846 53.57142857142857 62 45 106 84\n",
      "Image_23980 34.51327433628319 55.69620253164557 39 44 113 79\n",
      "Image_23982 22.641509433962263 28.125 24 27 106 96\n",
      "Image_23986 4.25531914893617 9.23076923076923 4 6 94 65\n",
      "Image_23988 7.03125 11.39240506329114 9 9 128 79\n",
      "Image_23989 17.24137931034483 13.513513513513514 10 5 58 37\n",
      "Image_23990 4.166666666666667 3.125 2 1 48 32\n",
      "Image_23991 5.128205128205129 3.225806451612903 2 1 39 31\n",
      "Image_23994 38.983050847457626 24.489795918367346 46 24 118 98\n",
      "Image_23995 15.714285714285714 15.517241379310345 11 9 70 58\n",
      "6.909090909090909 0.3145640909729906 23.454545454545453 0.2576302349189262 5.839181738043454 0.3625103494389097\n",
      "----------------------------------------\n",
      "0.3 0.5\n",
      "---------------------------------------------\n",
      "Image_23978 53.225806451612904 68.57142857142857 66 48 124 70\n",
      "Image_23979 58.490566037735846 65.21739130434783 62 45 106 69\n",
      "Image_23980 34.51327433628319 73.33333333333333 39 44 113 60\n",
      "Image_23982 22.641509433962263 35.064935064935064 24 27 106 77\n",
      "Image_23986 4.25531914893617 12.5 4 6 94 48\n",
      "Image_23988 7.03125 15.517241379310345 9 9 128 58\n",
      "Image_23989 17.24137931034483 15.625 10 5 58 32\n",
      "Image_23990 4.166666666666667 3.4482758620689653 2 1 48 29\n",
      "Image_23991 5.128205128205129 3.8461538461538463 2 1 39 26\n",
      "Image_23994 38.983050847457626 30.0 46 24 118 80\n",
      "Image_23995 15.714285714285714 18.75 11 9 70 48\n",
      "6.909090909090909 0.3145640909729906 37.0 0.3979227975025783 9.6074718886355 0.5600893881854666\n",
      "----------------------------------------\n",
      "0.5 0.1\n",
      "---------------------------------------------\n",
      "Image_23978 53.225806451612904 29.545454545454547 66 26 124 88\n",
      "Image_23979 58.490566037735846 24.719101123595507 62 22 106 89\n",
      "Image_23980 34.51327433628319 18.29268292682927 39 15 113 82\n",
      "Image_23982 22.641509433962263 11.538461538461538 24 12 106 104\n",
      "Image_23986 4.25531914893617 1.3513513513513513 4 1 94 74\n",
      "Image_23988 7.03125 3.409090909090909 9 3 128 88\n",
      "Image_23989 17.24137931034483 6.666666666666667 10 3 58 45\n",
      "Image_23990 4.166666666666667 0.0 2 0 48 38\n",
      "Image_23991 5.128205128205129 0.0 2 0 39 36\n",
      "Image_23994 38.983050847457626 7.766990291262136 46 8 118 103\n",
      "Image_23995 15.714285714285714 6.0606060606060606 11 4 70 66\n",
      "16.454545454545453 0.7223385246654405 17.363636363636363 0.17843872243620226 13.821900696561123 0.6553322462548711\n",
      "----------------------------------------\n",
      "0.5 0.3\n",
      "---------------------------------------------\n",
      "Image_23978 53.225806451612904 29.885057471264368 66 26 124 87\n",
      "Image_23979 58.490566037735846 26.19047619047619 62 22 106 84\n",
      "Image_23980 34.51327433628319 18.9873417721519 39 15 113 79\n",
      "Image_23982 22.641509433962263 12.5 24 12 106 96\n",
      "Image_23986 4.25531914893617 1.5384615384615385 4 1 94 65\n",
      "Image_23988 7.03125 3.7974683544303796 9 3 128 79\n",
      "Image_23989 17.24137931034483 8.108108108108109 10 3 58 37\n",
      "Image_23990 4.166666666666667 0.0 2 0 48 32\n",
      "Image_23991 5.128205128205129 0.0 2 0 39 31\n",
      "Image_23994 38.983050847457626 8.16326530612245 46 8 118 98\n",
      "Image_23995 15.714285714285714 6.896551724137931 11 4 70 58\n",
      "16.454545454545453 0.7223385246654405 23.454545454545453 0.2576302349189262 13.211325691848858 0.624395547338448\n",
      "----------------------------------------\n",
      "0.5 0.5\n",
      "---------------------------------------------\n",
      "Image_23978 53.225806451612904 37.142857142857146 66 26 124 70\n",
      "Image_23979 58.490566037735846 31.884057971014492 62 22 106 69\n",
      "Image_23980 34.51327433628319 25.0 39 15 113 60\n",
      "Image_23982 22.641509433962263 15.584415584415584 24 12 106 77\n",
      "Image_23986 4.25531914893617 2.0833333333333335 4 1 94 48\n",
      "Image_23988 7.03125 5.172413793103448 9 3 128 58\n",
      "Image_23989 17.24137931034483 9.375 10 3 58 32\n",
      "Image_23990 4.166666666666667 0.0 2 0 48 29\n",
      "Image_23991 5.128205128205129 0.0 2 0 39 26\n",
      "Image_23994 38.983050847457626 10.0 46 8 118 80\n",
      "Image_23995 15.714285714285714 8.333333333333334 11 4 70 48\n",
      "16.454545454545453 0.7223385246654405 37.0 0.3979227975025783 10.619627447039363 0.5262353648467776\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# pick one image from the test set\n",
    "import numpy as np\n",
    "fold_dir = '/home/aayush/chirag/tensorflow/yolo_kfold/2/images/'\n",
    "\n",
    "dataset_val = IELDatasetTest(os.path.join(fold_dir,\"val\"), 640, 640, transforms= get_transform(train=False))\n",
    "dataset_test = IELDatasetTest(os.path.join(fold_dir,\"test\"), 640, 640, transforms= get_transform(train=False))\n",
    "\n",
    "# put the model in evaluation mode\n",
    "model.eval()\n",
    "\n",
    "for iel_thres in [0.1,0.3,0.5]:\n",
    "    for epith_thres in [0.1,0.3,0.5]:\n",
    "        print(iel_thres , epith_thres)\n",
    "        print('---------------------------------------------')\n",
    "        \n",
    "        target_dict = {}\n",
    "        pred_dict = {}\n",
    "\n",
    "        for i in range(dataset_test.__len__()):\n",
    "            img,target = dataset_test[i]\n",
    "            image_name = target[\"image_name\"][:-6]\n",
    "\n",
    "            with torch.no_grad():\n",
    "                prediction = model([img.to(device)])[0]\n",
    "\n",
    "            nms_prediction = apply_nms(prediction, iou_thresh=0.01)\n",
    "\n",
    "            target_labels = target[\"labels\"].numpy()\n",
    "            pred_labels = list(nms_prediction[\"labels\"].cpu())\n",
    "            pred_scores = list(nms_prediction[\"scores\"].cpu())\n",
    "            num_iels = 0\n",
    "            num_epith = 0\n",
    "\n",
    "            for i in range(len(pred_labels)):\n",
    "                if pred_labels[i] == 1:\n",
    "                    if pred_scores[i] >= epith_thres:\n",
    "                        num_epith += 1\n",
    "                else:\n",
    "                    if pred_scores[i] >= iel_thres:\n",
    "                        num_iels += 1\n",
    "\n",
    "            if image_name in target_dict:\n",
    "                target_dict[image_name][\"iel\"] += np.count_nonzero(target_labels == 2)\n",
    "                target_dict[image_name][\"epith\"] += np.count_nonzero(target_labels == 1)\n",
    "                pred_dict[image_name][\"iel\"] += num_iels\n",
    "                pred_dict[image_name][\"epith\"] += num_epith\n",
    "\n",
    "            else:\n",
    "                target_dict[image_name] = {}\n",
    "                target_dict[image_name][\"iel\"] = np.count_nonzero(target_labels == 2)\n",
    "                target_dict[image_name][\"epith\"] = np.count_nonzero(target_labels == 1)\n",
    "\n",
    "                pred_dict[image_name] = {}\n",
    "                pred_dict[image_name][\"iel\"] = num_iels\n",
    "                pred_dict[image_name][\"epith\"] = num_epith\n",
    "\n",
    "        mae_iel = 0\n",
    "        mre_iel = 0\n",
    "        mae_epith = 0\n",
    "        mre_epith = 0\n",
    "        num_images = 0\n",
    "        mae_ratio = 0\n",
    "        mre_ratio = 0\n",
    "\n",
    "        for key in target_dict.keys():\n",
    "            num_images += 1\n",
    "            #print(key , target_dict[key][\"iel\"] , pred_dict[key][\"iel\"], target_dict[key][\"epith\"], pred_dict[key][\"epith\"] , target_dict[key][\"iel\"]/target_dict[key][\"epith\"])\n",
    "            mae_iel += abs(target_dict[key][\"iel\"] - pred_dict[key][\"iel\"])\n",
    "            mre_iel += abs(target_dict[key][\"iel\"] - pred_dict[key][\"iel\"])/target_dict[key][\"iel\"]\n",
    "            mae_epith += abs(target_dict[key][\"epith\"] - pred_dict[key][\"epith\"])\n",
    "            mre_epith += abs(target_dict[key][\"epith\"] - pred_dict[key][\"epith\"])/target_dict[key][\"epith\"]\n",
    "            \n",
    "            if pred_dict[key][\"epith\"] != 0:\n",
    "                print(key, (100*target_dict[key][\"iel\"])/target_dict[key][\"epith\"], (100*pred_dict[key][\"iel\"])/pred_dict[key][\"epith\"], target_dict[key][\"iel\"], pred_dict[key][\"iel\"], target_dict[key][\"epith\"], pred_dict[key][\"epith\"])\n",
    "                mae_ratio += abs((100*target_dict[key][\"iel\"])/target_dict[key][\"epith\"] - (100*pred_dict[key][\"iel\"])/pred_dict[key][\"epith\"])\n",
    "                \n",
    "                if target_dict[key][\"iel\"] != 0:\n",
    "                    mre_ratio += ((abs((100*target_dict[key][\"iel\"])/target_dict[key][\"epith\"] - (100*pred_dict[key][\"iel\"])/pred_dict[key][\"epith\"])) / ((100*target_dict[key][\"iel\"])/target_dict[key][\"epith\"]))\n",
    "            else:\n",
    "                print('Not nice :' ,key, target_dict[key][\"iel\"], pred_dict[key][\"iel\"], target_dict[key][\"epith\"], pred_dict[key][\"epith\"])\n",
    "\n",
    "        mae_iel /= num_images\n",
    "        mre_iel /= num_images\n",
    "        mae_epith /= num_images\n",
    "        mre_epith /= num_images\n",
    "        mae_ratio /= num_images\n",
    "        mre_ratio /= num_images\n",
    "        \n",
    "        \n",
    "        print(mae_iel , mre_iel , mae_epith , mre_epith , mae_ratio, mre_ratio)\n",
    "        print('----------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def box_iou(box1, box2):\n",
    "    # https://github.com/pytorch/vision/blob/master/torchvision/ops/boxes.py\n",
    "    \"\"\"\n",
    "    Return intersection-over-union (Jaccard index) of boxes.\n",
    "    Both sets of boxes are expected to be in (x1, y1, x2, y2) format.\n",
    "    Arguments:\n",
    "        box1 (Tensor[N, 4])\n",
    "        box2 (Tensor[M, 4])\n",
    "    Returns:\n",
    "        iou (Tensor[N, M]): the NxM matrix containing the pairwise\n",
    "            IoU values for every element in boxes1 and boxes2\n",
    "    \"\"\"\n",
    "\n",
    "    def box_area(box):\n",
    "        # box = 4xn\n",
    "        return (box[2] - box[0]) * (box[3] - box[1])\n",
    "\n",
    "    area1 = box_area(box1.T)\n",
    "    area2 = box_area(box2.T)\n",
    "\n",
    "    # inter(N,M) = (rb(N,M,2) - lt(N,M,2)).clamp(0).prod(2)\n",
    "    inter = (torch.min(box1[:, None, 2:], box2[:, 2:]) - torch.max(box1[:, None, :2], box2[:, :2])).clamp(0).prod(2)\n",
    "    return inter / (area1[:, None] + area2 - inter)  # iou = inter / (area1 + area2 - inter)\n",
    "\n",
    "def process_batch(detections, labels, iouv):\n",
    "    \"\"\"\n",
    "    Return correct predictions matrix. Both sets of boxes are in (x1, y1, x2, y2) format.\n",
    "    Arguments:\n",
    "        detections (Array[N, 6]), x1, y1, x2, y2, conf, class\n",
    "        labels (Array[M, 5]), class, x1, y1, x2, y2\n",
    "    Returns:\n",
    "        correct (Array[N, 10]), for 10 IoU levels\n",
    "    \"\"\"\n",
    "    correct = torch.zeros(detections.shape[0], iouv.shape[0], dtype=torch.bool, device=iouv.device)\n",
    "    iou = box_iou(labels[:, 1:], detections[:, :4])\n",
    "    x = torch.where((iou >= iouv[0]) & (labels[:, 0:1] == detections[:, 5]))  # IoU above threshold and classes match\n",
    "    if x[0].shape[0]:\n",
    "        matches = torch.cat((torch.stack(x, 1), iou[x[0], x[1]][:, None]), 1).cpu().numpy()  # [label, detection, iou]\n",
    "        if x[0].shape[0] > 1:\n",
    "            matches = matches[matches[:, 2].argsort()[::-1]]\n",
    "            matches = matches[np.unique(matches[:, 1], return_index=True)[1]]\n",
    "            # matches = matches[matches[:, 2].argsort()[::-1]]\n",
    "            matches = matches[np.unique(matches[:, 0], return_index=True)[1]]\n",
    "        matches = torch.Tensor(matches).to(iouv.device)\n",
    "        correct[matches[:, 1].long()] = matches[:, 2:3] >= iouv\n",
    "    return correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_ap(recall, precision):\n",
    "    \"\"\" Compute the average precision, given the recall and precision curves\n",
    "    # Arguments\n",
    "        recall:    The recall curve (list)\n",
    "        precision: The precision curve (list)\n",
    "    # Returns\n",
    "        Average precision, precision curve, recall curve\n",
    "    \"\"\"\n",
    "\n",
    "    # Append sentinel values to beginning and end\n",
    "    mrec = np.concatenate(([0.0], recall, [1.0]))\n",
    "    mpre = np.concatenate(([1.0], precision, [0.0]))\n",
    "\n",
    "    # Compute the precision envelope\n",
    "    mpre = np.flip(np.maximum.accumulate(np.flip(mpre)))\n",
    "\n",
    "    # Integrate area under curve\n",
    "    method = 'interp'  # methods: 'continuous', 'interp'\n",
    "    if method == 'interp':\n",
    "        x = np.linspace(0, 1, 101)  # 101-point interp (COCO)\n",
    "        ap = np.trapz(np.interp(x, mrec, mpre), x)  # integrate\n",
    "    else:  # 'continuous'\n",
    "        i = np.where(mrec[1:] != mrec[:-1])[0]  # points where x axis (recall) changes\n",
    "        ap = np.sum((mrec[i + 1] - mrec[i]) * mpre[i + 1])  # area under curve\n",
    "\n",
    "    return ap, mpre, mrec\n",
    "\n",
    "def ap_per_class(tp, conf, pred_cls, target_cls, plot=False, save_dir='.', names=(), eps=1e-16):\n",
    "    \"\"\" Compute the average precision, given the recall and precision curves.\n",
    "    Source: https://github.com/rafaelpadilla/Object-Detection-Metrics.\n",
    "    # Arguments\n",
    "        tp:  True positives (nparray, nx1 or nx10).\n",
    "        conf:  Objectness value from 0-1 (nparray).\n",
    "        pred_cls:  Predicted object classes (nparray).\n",
    "        target_cls:  True object classes (nparray).\n",
    "        plot:  Plot precision-recall curve at mAP@0.5\n",
    "        save_dir:  Plot save directory\n",
    "    # Returns\n",
    "        The average precision as computed in py-faster-rcnn.\n",
    "    \"\"\"\n",
    "\n",
    "    # Sort by objectness\n",
    "    i = np.argsort(-conf)\n",
    "    tp, conf, pred_cls = tp[i], conf[i], pred_cls[i]\n",
    "\n",
    "    # Find unique classes\n",
    "    unique_classes, nt = np.unique(target_cls, return_counts=True)\n",
    "    nc = unique_classes.shape[0]  # number of classes, number of detections\n",
    "\n",
    "    # Create Precision-Recall curve and compute AP for each class\n",
    "    px, py = np.linspace(0, 1, 1000), []  # for plotting\n",
    "    ap, p, r = np.zeros((nc, tp.shape[1])), np.zeros((nc, 1000)), np.zeros((nc, 1000))\n",
    "    for ci, c in enumerate(unique_classes):\n",
    "        i = pred_cls == c\n",
    "        n_l = nt[ci]  # number of labels\n",
    "        n_p = i.sum()  # number of predictions\n",
    "\n",
    "        if n_p == 0 or n_l == 0:\n",
    "            continue\n",
    "        else:\n",
    "            # Accumulate FPs and TPs\n",
    "            fpc = (1 - tp[i]).cumsum(0)\n",
    "            tpc = tp[i].cumsum(0)\n",
    "\n",
    "            # Recall\n",
    "            recall = tpc / (n_l + eps)  # recall curve\n",
    "            r[ci] = np.interp(-px, -conf[i], recall[:, 0], left=0)  # negative x, xp because xp decreases\n",
    "\n",
    "            # Precision\n",
    "            precision = tpc / (tpc + fpc)  # precision curve\n",
    "            p[ci] = np.interp(-px, -conf[i], precision[:, 0], left=1)  # p at pr_score\n",
    "\n",
    "            # AP from recall-precision curve\n",
    "            for j in range(tp.shape[1]):\n",
    "                ap[ci, j], mpre, mrec = compute_ap(recall[:, j], precision[:, j])\n",
    "                if plot and j == 0:\n",
    "                    py.append(np.interp(px, mrec, mpre))  # precision at mAP@0.5\n",
    "\n",
    "    # Compute F1 (harmonic mean of precision and recall)\n",
    "    f1 = 2 * p * r / (p + r + eps)\n",
    "\n",
    "    i = f1.mean(0).argmax()  # max F1 index\n",
    "    p, r, f1 = p[:, i], r[:, i], f1[:, i]\n",
    "    tp = (r * nt).round()  # true positives\n",
    "    fp = (tp / (p + eps) - tp).round()  # false positives\n",
    "    return tp, fp, p, r, f1, ap, unique_classes.astype('int32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([12, 5]) torch.Size([14, 6])\n",
      "torch.Size([47, 5]) torch.Size([36, 6])\n",
      "torch.Size([7, 5]) torch.Size([7, 6])\n",
      "torch.Size([15, 5]) torch.Size([11, 6])\n",
      "torch.Size([63, 5]) torch.Size([40, 6])\n",
      "torch.Size([21, 5]) torch.Size([14, 6])\n",
      "torch.Size([25, 5]) torch.Size([17, 6])\n",
      "torch.Size([18, 5]) torch.Size([19, 6])\n",
      "torch.Size([15, 5]) torch.Size([14, 6])\n",
      "torch.Size([2, 5]) torch.Size([5, 6])\n",
      "torch.Size([64, 5]) torch.Size([37, 6])\n",
      "torch.Size([48, 5]) torch.Size([40, 6])\n",
      "torch.Size([15, 5]) torch.Size([18, 6])\n",
      "torch.Size([6, 5]) torch.Size([8, 6])\n",
      "torch.Size([23, 5]) torch.Size([17, 6])\n",
      "torch.Size([52, 5]) torch.Size([43, 6])\n",
      "torch.Size([12, 5]) torch.Size([12, 6])\n",
      "torch.Size([31, 5]) torch.Size([28, 6])\n",
      "torch.Size([33, 5]) torch.Size([29, 6])\n",
      "torch.Size([1, 5]) torch.Size([3, 6])\n",
      "torch.Size([26, 5]) torch.Size([25, 6])\n",
      "torch.Size([14, 5]) torch.Size([18, 6])\n",
      "torch.Size([26, 5]) torch.Size([32, 6])\n",
      "torch.Size([48, 5]) torch.Size([38, 6])\n",
      "torch.Size([12, 5]) torch.Size([10, 6])\n",
      "torch.Size([2, 5]) torch.Size([5, 6])\n",
      "torch.Size([2, 5]) torch.Size([5, 6])\n",
      "torch.Size([1, 5]) torch.Size([7, 6])\n",
      "torch.Size([27, 5]) torch.Size([23, 6])\n",
      "torch.Size([2, 5]) torch.Size([4, 6])\n",
      "torch.Size([49, 5]) torch.Size([36, 6])\n",
      "torch.Size([16, 5]) torch.Size([9, 6])\n",
      "torch.Size([3, 5]) torch.Size([8, 6])\n",
      "torch.Size([63, 5]) torch.Size([37, 6])\n",
      "torch.Size([10, 5]) torch.Size([12, 6])\n",
      "torch.Size([4, 5]) torch.Size([6, 6])\n",
      "torch.Size([35, 5]) torch.Size([22, 6])\n",
      "torch.Size([22, 5]) torch.Size([16, 6])\n",
      "torch.Size([2, 5]) torch.Size([3, 6])\n",
      "torch.Size([1, 5]) torch.Size([4, 6])\n",
      "torch.Size([9, 5]) torch.Size([7, 6])\n",
      "torch.Size([32, 5]) torch.Size([19, 6])\n",
      "torch.Size([2, 5]) torch.Size([4, 6])\n",
      "torch.Size([22, 5]) torch.Size([18, 6])\n",
      "torch.Size([3, 5]) torch.Size([5, 6])\n",
      "torch.Size([21, 5]) torch.Size([17, 6])\n",
      "torch.Size([15, 5]) torch.Size([14, 6])\n",
      "torch.Size([14, 5]) torch.Size([18, 6])\n",
      "torch.Size([18, 5]) torch.Size([17, 6])\n",
      "torch.Size([12, 5]) torch.Size([10, 6])\n",
      "torch.Size([8, 5]) torch.Size([9, 6])\n",
      "torch.Size([3, 5]) torch.Size([5, 6])\n",
      "torch.Size([30, 5]) torch.Size([28, 6])\n",
      "torch.Size([6, 5]) torch.Size([7, 6])\n",
      "torch.Size([36, 5]) torch.Size([32, 6])\n",
      "torch.Size([52, 5]) torch.Size([38, 6])\n",
      "torch.Size([7, 5]) torch.Size([6, 6])\n",
      "torch.Size([33, 5]) torch.Size([23, 6])\n",
      "torch.Size([15, 5]) torch.Size([14, 6])\n",
      "torch.Size([15, 5]) torch.Size([18, 6])\n",
      "torch.Size([15, 5]) torch.Size([18, 6])\n",
      "torch.Size([21, 5]) torch.Size([22, 6])\n",
      "torch.Size([1, 5]) torch.Size([1, 6])\n",
      "torch.Size([14, 5]) torch.Size([9, 6])\n",
      "0.6859231765977765 0.5010931872520463 0.603925842876629 0.39217818831453144\n"
     ]
    }
   ],
   "source": [
    "# pick one image from the test set\n",
    "import numpy as np\n",
    "\n",
    "dataset_val = IELDatasetTest(os.path.join(fold_dir,\"val\"), 640, 640, transforms= get_transform(train=False))\n",
    "dataset_test = IELDatasetTest(os.path.join(fold_dir,\"test\"), 640, 640, transforms= get_transform(train=False))\n",
    "\n",
    "# put the model in evaluation mode\n",
    "model.eval()\n",
    "\n",
    "mAPl = 0.3\n",
    "mAPr = 0.75\n",
    "iouv = torch.linspace(mAPl, mAPr, 10).to(device)  # iou vector for mAP@0.5:0.\n",
    "niou = iouv.numel()\n",
    "\n",
    "jdict, stats, ap, ap_class = [], [], [], []\n",
    "\n",
    "for i in range(dataset_test.__len__()):\n",
    "    img,target = dataset_test[i]\n",
    "    image_name = target[\"image_name\"][:-6]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        prediction = model([img.to(device)])[0]\n",
    "\n",
    "    nms_prediction = apply_nms(prediction, iou_thresh=0.01)\n",
    "    \n",
    "    labelsn = torch.cat((torch.reshape(target[\"labels\"],(target[\"labels\"].shape[0],1)), target[\"boxes\"]), 1)\n",
    "    labelsn = labelsn.to(device)\n",
    "    \n",
    "    pred_boxes = nms_prediction[\"boxes\"]\n",
    "    pred_labels = nms_prediction[\"labels\"]\n",
    "    pred_conf = nms_prediction[\"scores\"]\n",
    "    \n",
    "    predn = torch.cat((pred_boxes, torch.reshape(pred_conf , (pred_conf.shape[0],1)) , torch.reshape(pred_labels, (pred_labels.shape[0],1))), 1)\n",
    "    \n",
    "    print(labelsn.shape,predn.shape)\n",
    "    \n",
    "    correct = process_batch(predn, labelsn, iouv)\n",
    "    \n",
    "    stats.append((correct.cpu(), predn[:, 4].cpu(), predn[:, 5].cpu(), target[\"labels\"].tolist()))  # (correct, conf, pcls, tcls)\n",
    "\n",
    "stats = [np.concatenate(x, 0) for x in zip(*stats)]  # to numpy\n",
    "if len(stats) and stats[0].any():\n",
    "    tp, fp, p, r, f1, ap, ap_class = ap_per_class(*stats)\n",
    "    ap50, ap = ap[:, 0], ap.mean(1)  # AP@0.5, AP@0.5:0.95\n",
    "    mp, mr, map50, map5095 = p.mean(), r.mean(), ap50.mean(), ap.mean()\n",
    "    \n",
    "print(mp, mr, map50, map5095)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "fce2ee5fb604e705f44e6b2ab28ec7fde5700589e46fa3e6385091720f01289d"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
